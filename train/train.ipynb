{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd18d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "! echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eddc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b17346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LD_LIBRARY_PATH\"] = '/usr/lib/wsl/lib:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc9f38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf39cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d7dce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A5000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eee8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../dataset/safegraph/utils/')\n",
    "sys.path.append('utils/')\n",
    "from dataset_classes import * \n",
    "from models import *\n",
    "from helper_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625cbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(loss, metric, best_epoch_loss, best_epoch_metric):\n",
    "    plt.figure()\n",
    "    plt.plot([epoch for epoch in range(0, len(loss))], loss, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Train Stage: {data_type}')\n",
    "    plt.axvline(x = best_epoch_loss, color = 'r', linestyle='-')\n",
    "    plt.savefig(save_dir + 'loss')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([epoch for epoch in range(0, len(metric))], metric, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (of triplet learning)')\n",
    "    plt.title(f'Train Stage: {data_type}')\n",
    "    plt.axvline(x = best_epoch_metric, color = 'r', linestyle='-')\n",
    "    plt.savefig(save_dir + 'accuracy')\n",
    "    \n",
    "def metrics(stats):\n",
    "    \"\"\"\n",
    "    Self-defined metrics function to evaluate and compare models\n",
    "    stats: {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN}\n",
    "    return: must be a single number \"\"\"\n",
    "    accuracy = (stats['T'] + 0.00001) * 1.0 / (stats['T'] + stats['F'] + 0.00001)\n",
    "    # print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained= None#'outputs/landsat_1000/landsat_30_last.tar' #None # last model save\n",
    "stage = 'landsat/'#'mobility_1051/' #'mapillary/'#'landsat/'\n",
    "save_dir = 'outputs/' + stage \n",
    "data_type = 'landsat'#'distance' # 'mobility' #'poi' #'sv'\n",
    "save_path = save_dir + \"training_log_\" + data_type + \".txt\"\n",
    "node_list_path='../dataset/safegraph/graph_checkpoints/nyc_metro/node_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9366e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    createCleanDir(save_dir)\n",
    "\n",
    "with open(save_path, \"w\") as file:\n",
    "    file.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aaeea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1089\n"
     ]
    }
   ],
   "source": [
    "# device = \"cpu\"\n",
    "embedding_dim = 200\n",
    "num_nodes = pd.read_csv(node_list_path).shape[0]\n",
    "print(f'Number of nodes: {num_nodes}')\n",
    "threshold = 0.6\n",
    "return_best = True\n",
    "if_early_stop = True\n",
    "input_size = 299\n",
    "learning_rate = [0.008]\n",
    "weight_decay = [0.0005]\n",
    "batch_size = 2\n",
    "num_epochs = 60\n",
    "lr_decay_rate = 0.7\n",
    "lr_decay_epochs = 6\n",
    "early_stop_epochs = 14\n",
    "save_epochs = 3\n",
    "margin=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ece50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class ResNetTransform:\n",
    "    \"\"\"Feed images through ResNet-18 model to get embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = ResNet18_Weights.DEFAULT\n",
    "        model = resnet18(weights=weights, progress=False)\n",
    "        num_features = model.fc.in_features \n",
    "        model.fc = nn.Linear(num_features, 200)\n",
    "#         model = FullyConvolutionalResnet18(pretrained=True)\n",
    "\n",
    "        self.resnet18 = model.eval()\n",
    "        self.resnet18 = self.resnet18.to(device)\n",
    "        \n",
    "        self.transforms =  torchvision.transforms.Compose([\n",
    "#             torchvision.transforms.Resize((224, 224)),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "\n",
    "    def __call__(self, x):\n",
    "        print('Begin')\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x)\n",
    "            x = torch.nan_to_num(x, 0, 0, 0)\n",
    "#             print(x.shape)\n",
    "            x = x.to(device)\n",
    "            print('1')\n",
    "            print(type(x))\n",
    "            print(x.shape)\n",
    "            x = self.transforms(x)\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "#             print(x.shape)\n",
    "            with torch.no_grad():\n",
    "#                 x = x.to(device)\n",
    "                print('into model')\n",
    "                print(x.shape)\n",
    "                y_pred = self.resnet18(x)\n",
    "                print('out of model')\n",
    "                y_pred = torch.softmax(y_pred, dim=1)\n",
    "            y_pred = torch.squeeze(y_pred)\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ea51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landsat_model = ResNetTransform()\n",
    "# # landsat_model.to(device)\n",
    "# datasets1 = SatelliteImageryDataset(node_list_path=node_list_path, \n",
    "#                                     root_image_dir='../dataset/earth_engine/download_landsat_images', \n",
    "#                                     fn='least_cloudy_rectangle_highres.tif', is_train=True, transform=landsat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b062e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 235, 378)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1= datasets1.__getsampletest__(300)\n",
    "sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b4d59dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 235, 378])\n"
     ]
    }
   ],
   "source": [
    "x = sample1\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor(x)\n",
    "    x = torch.nan_to_num(x, 0, 0, 0)\n",
    "    x = x.to(device)\n",
    "    print('1')\n",
    "    print(type(x))\n",
    "    print(x.shape)\n",
    "    out = landsat_model.transforms(x).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49488fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 235, 378])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_model.resnet18(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d749e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "landsat_mode.unsqueeze(0)l.__call__(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b46d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(299),\n",
    "#     transforms.CenterCrop(299),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# class EncoderInception3(nn.Module):\n",
    "#     def __init__(self, embedding_dim=200):\n",
    "#         super(EncoderInception3, self).__init__()\n",
    "#         self.model = torchvision.models.inception_v3(weights='DEFAULT').eval()\n",
    "#         self.set_parameter_requires_grad(True)  # freeze all but last layer\n",
    "\n",
    "#         # Handle the auxilary net\n",
    "#         self.model.aux_logits = False\n",
    "\n",
    "#         # Handle the primary net\n",
    "#         num_ftrs = self.model.fc.in_features\n",
    "#         self.model.fc = nn.Linear(num_ftrs, embedding_dim)\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         x = self.model(images)  \n",
    "#         return x\n",
    "\n",
    "#     def set_parameter_requires_grad(self, feature_extracting):\n",
    "#         if feature_extracting:\n",
    "#             for param in self.model.parameters():\n",
    "#                 param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5adade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "class Inceptionv3Transform:\n",
    "    \"\"\"Feed images through ResNet-18 model to get embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = Inception_V3_Weights.DEFAULT\n",
    "        model = inception_v3(weights=weights, progress=False)\n",
    "        num_features = model.fc.in_features \n",
    "        model.fc = nn.Linear(num_features, 200)\n",
    "        self.inception_v3 = model.eval()\n",
    "        self.inception_v3 = self.inception_v3.to(device)\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "            \n",
    "\n",
    "    def __call__(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x).float()\n",
    "            x = torch.nan_to_num(x, 0, 0, 0)\n",
    "        \n",
    "            if len(x.shape) != 3:\n",
    "                print(x.shape)\n",
    "                print(x)\n",
    "            try:\n",
    "                x = torch.reshape(x, (3, x.shape[0], x.shape[1]))\n",
    "            except:\n",
    "                print(x.shape)\n",
    "                x = torch.reshape(x, (x.shape[2], x.shape[0], x.shape[1]))\n",
    "            x = self.transforms(x)\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "            with torch.no_grad():\n",
    "#                 print(x.shape)\n",
    "                x = x.to(device)\n",
    "                y_pred = self.inception_v3(x)\n",
    "            y_pred = torch.squeeze(y_pred)\n",
    "            if(not np.isfinite(y_pred.cpu().detach().numpy()).all()):\n",
    "                print('Invalid num in inception transform: {y_pred}')\n",
    "            return y_pred\n",
    "\n",
    "# torch.Size([3, 310, 306])\n",
    "# torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbdcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sv_model = Inceptionv3Transform()\n",
    "# datasets1 = StreetViewDataset(node_list_path=node_list_path, \n",
    "#                                     root_image_dir='../dataset/mapillary/data/nyc_metro/', \n",
    "#                               is_train=True, transform=sv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf928ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(model, data_type, dataloaders, criterion, optimizer, metrics, num_epochs,\n",
    "                verbose=True, return_best=True, if_early_stop=True, early_stop_epochs=10, scheduler=None,\n",
    "                save_dir=None, save_epochs=5):\n",
    "    since = time.time()\n",
    "    training_log = dict()\n",
    "    training_log['loss_history'] = []\n",
    "    training_log['metric_value_history'] = []\n",
    "    training_log['current_epoch'] = -1\n",
    "    current_epoch = training_log['current_epoch'] + 1\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_optimizer_wts = copy.deepcopy(optimizer.state_dict())\n",
    "    best_log = copy.deepcopy(training_log)\n",
    "\n",
    "    best_metric_value = 0\n",
    "    nodecrease = 0  # to count the epochs that val loss doesn't decrease\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in tqdm(range(current_epoch, current_epoch + num_epochs)):\n",
    "        if verbose:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "        running_loss = 0.0\n",
    "        stats = {'T':0,'F':0}\n",
    "\n",
    "        # Iterate over data.\n",
    "        for pos_index, pos_encoder_embedding, neg_encoder_embedding in dataloaders:\n",
    "            # print(f\"Feature batch shape: {pos_index.size()}\")\n",
    "            # print(f\"Labels batch shape: {pos_encoder_embedding.size()}\")\n",
    "\n",
    "            pos_encoder_embedding = pos_encoder_embedding.to(device)\n",
    "            neg_encoder_embedding = neg_encoder_embedding.to(device)\n",
    "            pos_index = pos_index.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Get model outputs and calculate loss\n",
    "                outputs = model(pos_index)\n",
    "\n",
    "                loss = criterion(outputs, pos_encoder_embedding, neg_encoder_embedding) # anchor, pos, neg\n",
    "\n",
    "                # evaluate based on whether dist(anchor, pos) < dist(anchor, neg)\n",
    "                pos_dist = torch.nn.functional.pairwise_distance(outputs, pos_encoder_embedding, p=2.0) \n",
    "                neg_dist = torch.nn.functional.pairwise_distance(outputs, neg_encoder_embedding, p=2.0) \n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "            stats['T'] += torch.sum(pos_dist < neg_dist).cpu().item() # if pos distance < neg distance, this is good\n",
    "            stats['F'] += torch.sum(pos_dist > neg_dist).cpu().item()\n",
    "\n",
    "       \n",
    "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "        epoch_metric_value = metrics(stats)\n",
    "        if verbose:\n",
    "            print('Loss: {:.5f} Metrics: {:.5f}'.format( epoch_loss, epoch_metric_value))\n",
    "\n",
    "        training_log['current_epoch'] = epoch\n",
    "        print()\n",
    "        training_log['metric_value_history'].append(epoch_metric_value)\n",
    "        training_log['loss_history'].append(epoch_loss)\n",
    "        if epoch_metric_value > best_metric_value:\n",
    "            best_metric_value = epoch_metric_value\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer_wts = copy.deepcopy(optimizer.state_dict())\n",
    "            best_log = copy.deepcopy(training_log)\n",
    "            nodecrease = 0\n",
    "        else:\n",
    "            nodecrease += 1\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "        if nodecrease >= early_stop_epochs:\n",
    "            early_stop = True\n",
    "        if save_dir and epoch % save_epochs == 0:\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'training_log': training_log\n",
    "                }\n",
    "            torch.save(checkpoint,os.path.join(save_dir, data_type + '_' + str(training_log['current_epoch']) + '.tar'))\n",
    "        if if_early_stop and early_stop:\n",
    "            print('Early stopped!')\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best metric value: {:5f}'.format(best_metric_value))\n",
    "\n",
    "    # plot full training curve\n",
    "    best_epoch_loss = np.argmin(training_log[\"loss_history\"])\n",
    "    best_epoch_metric = np.argmax(training_log[\"metric_value_history\"])\n",
    "    plot_curves(training_log[\"loss_history\"], training_log[\"metric_value_history\"], best_epoch_loss=best_epoch_loss, best_epoch_metric=best_epoch_metric)\n",
    "\n",
    "    # load best model weights\n",
    "    if return_best:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        optimizer.load_state_dict(best_optimizer_wts)\n",
    "        training_log = best_log\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_log': training_log\n",
    "    }\n",
    "\n",
    "    if save_dir:\n",
    "        torch.save(checkpoint,\n",
    "                   os.path.join(save_dir, data_type + '_' + str(training_log['current_epoch']) + '_last.tar'))    \n",
    "    return model, training_log, best_metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c283b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f4e7836a7d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1af072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1089, 200])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb78d8b30b04bddb94c11f084ba0c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "----------\n",
      "Begin\n",
      "1\n",
      "into model\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "\n",
    "\n",
    "best_metric=0\n",
    "best_lr=-1\n",
    "best_wr=-1\n",
    "\n",
    "for i in learning_rate:\n",
    "    for j in weight_decay:\n",
    "        dataloaders_dict = DataLoader(datasets1, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "        model = NodeEmbeddings(num_nodes, embedding_dim=200)\n",
    "        for param in model.parameters():\n",
    "            print(param.size())\n",
    "\n",
    "\n",
    "        model = model.to(device)\n",
    "        if pre_trained:\n",
    "            checkpoint=torch.load(pre_trained)\n",
    "            model.load_state_dict(checkpoint,strict=False)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=i, betas=(0.9, 0.999), eps=1e-08,\n",
    "                               weight_decay=j, amsgrad=True)\n",
    "        loss_fn = torch.nn.TripletMarginLoss(reduction=\"mean\", margin=margin)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_epochs, gamma=lr_decay_rate)\n",
    "\n",
    "        _, training_log,best_value = train_embedding(model, data_type=data_type, dataloaders=dataloaders_dict, criterion=loss_fn,\n",
    "                               optimizer=optimizer, metrics=metrics, num_epochs=num_epochs,\n",
    "                               save_dir=save_dir, verbose=True, return_best=return_best,\n",
    "                               if_early_stop=if_early_stop, early_stop_epochs=early_stop_epochs, scheduler=scheduler,\n",
    "                               save_epochs=save_epochs)\n",
    "        \n",
    "        \n",
    "        print(training_log[\"metric_value_history\"])\n",
    "\n",
    "    if best_value>best_metric:\n",
    "        best_metric=best_value\n",
    "        best_lr=i\n",
    "        best_wr=j\n",
    "\n",
    "    with open(save_dir + 'metric_value_history.npy', 'wb') as f:\n",
    "        np.save(f, training_log[\"metric_value_history\"])\n",
    "    with open(save_dir + 'loss_history.npy', 'wb') as f:\n",
    "        np.save(f, training_log[\"loss_history\"])\n",
    "\n",
    "    best_epoch_loss = np.argmax(training_log[\"loss_history\"])\n",
    "    best_epoch_metric = np.argmax(training_log[\"metric_value_history\"])\n",
    "    with open(save_path, \"a\") as file:\n",
    "        for k in range(len(training_log[\"metric_value_history\"])):\n",
    "            file.write(\"epoch:\" + str(k)+\"\\n\")\n",
    "            file.write(\"metric_value_history:\"+str(training_log[\"metric_value_history\"][k])+\"\\n\")\n",
    "            file.write(\"loss_history:\"+str(training_log[\"loss_history\"][k])+\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\\n---BEST---\\nbest_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "    print(\"best_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "    model.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc6fdcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = pos.cpu().detach()\n",
    "np.isnan(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec369d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(pos).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a96803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_image = io.imread('../dataset/mapillary/data/nyc_metro/36031961100/41.jpg')#.to_numpy(dtype='float64')\n",
    "\n",
    "# t = transforms.Compose([\n",
    "#             transforms.Resize(299),\n",
    "#             transforms.CenterCrop(299),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ])\n",
    "# x = np.array(pos_image)\n",
    "\n",
    "# x = torch.tensor(x).float()\n",
    "\n",
    "\n",
    "# x = x.to(device)\n",
    "# x = torch.nan_to_num(x, 0, 0, 0)\n",
    "\n",
    "# # x = x.float()\n",
    "\n",
    "# x = torch.reshape(x, (3, x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ea497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_metric=0\n",
    "# best_lr=-1\n",
    "# best_wr=-1\n",
    "\n",
    "# for i in learning_rate:\n",
    "#     for j in weight_decay:\n",
    "#         model = NodeEmbeddings(num_nodes, embedding_dim=200)\n",
    "#         for param in model.parameters():\n",
    "#             print(param.size())\n",
    "#         # media_dir=os.path.join(ckpt_save_dir,\"lr%f_wr%f\"%(i,j))\n",
    "#         # if not os.path.exists(media_dir):\n",
    "#         #     os.makedirs(media_dir)\n",
    "#         model = model.to(device)\n",
    "#         if pre_trained:\n",
    "#             checkpoint=torch.load(pre_trained)\n",
    "#             model.load_state_dict(checkpoint,strict=False)\n",
    "            \n",
    "#         optimizer = optim.Adam(model.parameters(), lr=i, betas=(0.9, 0.999), eps=1e-08,\n",
    "#                                weight_decay=j, amsgrad=True)\n",
    "#         loss_fn = torch.nn.TripletMarginLoss(reduction=\"mean\", margin=margin)\n",
    "#         scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_epochs, gamma=lr_decay_rate)\n",
    "\n",
    "#         _, training_log,best_value = train_embedding(model, data_type=data_type, dataloaders=dataloaders_dict, criterion=loss_fn,\n",
    "#                                optimizer=optimizer, metrics=metrics, num_epochs=num_epochs,\n",
    "#                                save_dir=save_dir, verbose=True, return_best=return_best,\n",
    "#                                if_early_stop=if_early_stop, early_stop_epochs=early_stop_epochs, scheduler=scheduler,\n",
    "#                                save_epochs=save_epochs)\n",
    "\n",
    "#         print(training_log[\"metric_value_history\"])\n",
    "\n",
    "#         if best_value>best_metric:\n",
    "#             best_metric=best_value\n",
    "#             best_lr=i\n",
    "#             best_wr=j\n",
    "\n",
    "#         with open(save_dir + 'metric_value_history.npy', 'wb') as f:\n",
    "#             np.save(f, training_log[\"metric_value_history\"])\n",
    "#         with open(save_dir + 'loss_history.npy', 'wb') as f:\n",
    "#             np.save(f, training_log[\"loss_history\"])\n",
    "\n",
    "#         best_epoch_loss = np.argmax(training_log[\"loss_history\"])\n",
    "#         best_epoch_metric = np.argmax(training_log[\"metric_value_history\"])\n",
    "#         with open(save_path, \"a\") as file:\n",
    "#             for k in range(len(training_log[\"metric_value_history\"])):\n",
    "#                 file.write(\"epoch:\" + str(k)+\"\\n\")\n",
    "#                 file.write(\"metric_value_history:\"+str(training_log[\"metric_value_history\"][k])+\"\\n\")\n",
    "#                 file.write(\"loss_history:\"+str(training_log[\"loss_history\"][k])+\"\\n\")\n",
    "\n",
    "#             file.write(\"\\n\\n---BEST---\\nbest_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "#         print(\"best_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "# model.print_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_log[\"metric_value_history\"])\n",
    "\n",
    "# if best_value>best_metric:\n",
    "#     best_metric=best_value\n",
    "#     best_lr=i\n",
    "#     best_wr=j\n",
    "\n",
    "# with open(save_dir + 'metric_value_history.npy', 'wb') as f:\n",
    "#     np.save(f, training_log[\"metric_value_history\"])\n",
    "# with open(save_dir + 'loss_history.npy', 'wb') as f:\n",
    "#     np.save(f, training_log[\"loss_history\"])\n",
    "\n",
    "# best_epoch_loss = np.argmax(training_log[\"loss_history\"])\n",
    "# best_epoch_metric = np.argmax(training_log[\"metric_value_history\"])\n",
    "# with open(save_path, \"a\") as file:\n",
    "#     for k in range(len(training_log[\"metric_value_history\"])):\n",
    "#         file.write(\"epoch:\" + str(k)+\"\\n\")\n",
    "#         file.write(\"metric_value_history:\"+str(training_log[\"metric_value_history\"][k])+\"\\n\")\n",
    "#         file.write(\"loss_history:\"+str(training_log[\"loss_history\"][k])+\"\\n\")\n",
    "\n",
    "#     file.write(\"\\n\\n---BEST---\\nbest_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "# print(\"best_lr:\"+str(best_lr)+\" best_wr:\"+str(best_wr)+\" best_metric_value:\"+str(best_metric))\n",
    "\n",
    "# model.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea9467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
